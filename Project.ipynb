{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "## Introduction:\n",
    "\n",
    "Uber announced that its users in New York City could order yellow taxis through the Uber app in the future. To explore the trends for Uber and yellow taxi, we make analysis based on hired-ride trip data from Uber and NYC Yellow cab from January 2009 through June 2015, and local historical weather data.\n",
    "##### The analysis is mainly broken up into 4 Parts: (Detailed analysis about each part are shown in the following Jupyter Notebook)\n",
    "<br>\n",
    "Data Preprocessing\n",
    "<br>\n",
    "Storing Data\n",
    "<br>\n",
    "Understanding Data\n",
    "<br>\n",
    "Visualizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup\n",
    "All import statements needed for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "from scipy.stats import sem\n",
    "from keplergl import KeplerGl\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "import geopandas as gpd\n",
    "import re\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.animation import FuncAnimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating distance\n",
    "Define a functin called \"calculate_distance\" that calculates the distance between two coordinates in kilometers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(from_lat, from_long,to_lat,to_long):\n",
    "    \"\"\"Calculate the distance bewteen two coordinates in kilometers.\n",
    "\n",
    "    Keyword arguments:\n",
    "    Inputs: \n",
    "        from_lat -- first coordinate's latitude\n",
    "        from_long -- first coordinate's longitude\n",
    "        to_lat -- second coordinate's latitude\n",
    "        to_long -- second coordinate's longitude\n",
    "    Output:\n",
    "        distance -- distance between two coordinates in kilometers\n",
    "    \"\"\"\n",
    "    \n",
    "    R = 6373.0\n",
    "    lat1 = radians(from_lat)\n",
    "    lon1 = radians(from_long)\n",
    "    lat2 = radians(to_lat)\n",
    "    lon2 = radians(to_long)\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Taxi Data\n",
    "\n",
    "For the taxi data, we first downloaded parquet files programmingly using regular expression,requests and beautifulsoup module. Then, we constructed a sampling of 3000 rows for each month data to make the sample size consistent with the one of Uber Data. The next step is cleaning data including removing invalid data, unnecessary columns, and add a column of distance according to the coordinates. Lastly, we append data of each month to a big dataframe.\n",
    "<br>\n",
    "\n",
    "#### Invalid Data Criteria:\n",
    "passenger count=0;\n",
    "fare amount<=0;\n",
    "distance <=0;\n",
    " coordinates out of New York box or NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Find Urls of Taxi Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_csv_urls():\n",
    "    \"\"\"Get Urls using requests and beautifulsoup\n",
    "    Keyword Arguments:\n",
    "    Output:\n",
    "        res -- A list contain all urls of yellow taxi montly data\n",
    "    \n",
    "    \"\"\"\n",
    "    TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "    response = requests.get(TAXI_URL)\n",
    "    html = response.content\n",
    "    soup = bs4.BeautifulSoup(html,'html.parser')\n",
    "    w=soup.find_all(\"a\")\n",
    "    res=[]\n",
    "    for i in range(len(w)):\n",
    "        if w[i].text==\"Yellow Taxi Trip Records\":\n",
    "            res.append(w[i]['href'])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Download Taxi Data from 2009-01 to 2015-06__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=find_taxi_csv_urls()\n",
    "for x in res:\n",
    "    #Use regular expression to extract required urls and use requests to download data\n",
    "    pattern = r\"(2009-\\d{2}|2010-\\d{2}|2011-\\d{2}|2012-\\d{2}|2013-\\d{2}|2014-\\d{2}|2015-0[1-6])\\.(parquet)\"\n",
    "    result = re.search(pattern, x)\n",
    "   \n",
    "    if result != None:\n",
    "        response = requests.get(x, stream=True)\n",
    "        title=result.groups()[0]\n",
    "    with open(title+\".parquet\", \"wb\") as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Clean Data from 2011 to 2015 (Location ID is provided instead of Coordinates)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = gpd.read_file('taxi_zones.shp')\n",
    "df2 = df2.to_crs(epsg=4326)  # EPSG 4326 = WGS84 = https://epsg.io/4326\n",
    "def find_long(ID,df2):\n",
    "    \"\"\"Find longitude using taxi_zones.shp corresponding to the Location ID\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        ID -- Location ID\n",
    "        df2 -- dataframe of taxi_zones.shp\n",
    "    \n",
    "    Outputs:\n",
    "        long-- longitude\n",
    "        np.nan-- if the coordinate is out of new york box\n",
    "    \"\"\"\n",
    "    long=df2.iloc[ID-1].geometry.centroid.x\n",
    "    if -74.242330<=long<=-73.717047:\n",
    "        return long\n",
    "    else:\n",
    "        return np.nan\n",
    "def find_lat(ID,df2):\n",
    "    \"\"\"Find latitude using taxi_zones.shp corresponding to the Location ID\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        ID -- Location ID\n",
    "        df2 -- dataframe of taxi_zones.shp \n",
    "    \n",
    "    Outputs:\n",
    "        lat-- latitude\n",
    "        np.nan-- if the coordinate is out of new york box\n",
    "    \"\"\"\n",
    "    lat=df2.iloc[ID-1].geometry.centroid.y\n",
    "    if 40.560445<=lat<=40.908524:\n",
    "        return lat\n",
    "    else:\n",
    "        return np.nan\n",
    "def normal_from_2011(parquet_file):\n",
    "    \"\"\"Clean Data: Normalize column names, remove invalid rows, sampling size=3000, find coordinates and add distance\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        parquet_file -- a string of parquet file's name\n",
    "    \n",
    "    Outputs:\n",
    "        taxi_df -- dataframe with columns of datetime, coordinates of pickup&dropoff, distance, and tip amount\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_parquet(parquet_file,engine='pyarrow')\n",
    "    df = df[(df.passenger_count != 0) & (df.fare_amount > 0)]\n",
    "    df.rename(columns={'tpep_pickup_datetime':'pickup_datetime'},inplace=True)    \n",
    "    df.set_index(pd.to_datetime(df[\"pickup_datetime\"]),inplace=True)\n",
    "    df = df[[\"PULocationID\", \"DOLocationID\",\"tip_amount\"]]\n",
    "    taxi_df=df.sample(n=3000,random_state=100)\n",
    "    taxi_df = taxi_df.loc[(taxi_df['PULocationID'] < 264) & (taxi_df['PULocationID'] >= 1)]\n",
    "    taxi_df = taxi_df.loc[(taxi_df['DOLocationID'] < 264) & (taxi_df['DOLocationID'] >= 1)]\n",
    "    \n",
    "    taxi_df[\"pickup_latitude\"]=np.nan\n",
    "    taxi_df[\"pickup_longitude\"]=np.nan\n",
    "    taxi_df[\"dropoff_latitude\"]=np.nan\n",
    "    taxi_df[\"dropoff_longitude\"]=np.nan\n",
    "    lat1 = taxi_df.apply(\n",
    "        lambda row: find_lat(row[\"PULocationID\"].astype('int'),df2),axis=1)\n",
    "    long1 = taxi_df.apply(\n",
    "        lambda row: find_long(row[\"PULocationID\"].astype('int'),df2),axis=1)\n",
    "    \n",
    "    taxi_df[\"pickup_latitude\"] = lat1\n",
    "    taxi_df[\"pickup_longitude\"] = long1\n",
    " \n",
    "    lat2 = taxi_df.apply(\n",
    "        lambda row: find_lat(row[\"DOLocationID\"].astype('int'),df2),axis=1)\n",
    "    long2 = taxi_df.apply(\n",
    "    lambda row: find_long(row[\"DOLocationID\"].astype('int'),df2),axis=1)\n",
    "\n",
    "    \n",
    "    taxi_df[\"dropoff_latitude\"] = lat2\n",
    "    taxi_df[\"dropoff_longitude\"] = long2   \n",
    "    \n",
    "    add_distance = taxi_df.apply(\n",
    "                 lambda row: calculate_distance(row[\"pickup_latitude\"], row[\"pickup_longitude\"],row[\"dropoff_latitude\"],row[\"dropoff_longitude\"]),\n",
    "                 axis=1)\n",
    "    taxi_df['distance'] = add_distance\n",
    "    taxi_df = taxi_df[taxi_df.distance > 0]\n",
    "    taxi_df.dropna(inplace=True)\n",
    "    \n",
    "    return pd.DataFrame(taxi_df, columns=[\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\",\"dropoff_latitude\",\"tip_amount\",\"distance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. Clean Data of 2009 and 2010 Respectively__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_2009(file):\n",
    "    \"\"\"Clean Data: extract necessary columns and normalize names, remove invalid rows, sample size=3000\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        file -- a string of parquet file's name\n",
    "    \n",
    "    Outputs:\n",
    "        par -- dataframe with columns of datetime, coordinates of pickup&dropoff and tip amount\n",
    "    \"\"\"\n",
    "    par=pd.read_parquet(file,engine='pyarrow')\n",
    "    par.rename(columns={'Trip_Pickup_DateTime':'pickup_datetime'},inplace=True)\n",
    "    par.set_index(pd.to_datetime(par['pickup_datetime']),inplace=True)\n",
    "    par = par[(par.Passenger_Count != 0) & (par.Fare_Amt > 0)]\n",
    "    par=par[['Start_Lon','Start_Lat','End_Lon','End_Lat','Tip_Amt']]\n",
    "    par=par.sample(3000,random_state=100)\n",
    "    par.rename(columns={'Start_Lon': 'pickup_longitude', 'Start_Lat': 'pickup_latitude','End_Lon':'dropoff_longitude','End_Lat':'dropoff_latitude','Tip_Amt':'tip_amount'},inplace=True)\n",
    "    return par\n",
    "def norm_2010(parquet_file): \n",
    "    \"\"\"Clean Data: extract necessary columns and normalize names, remove invalid rows, sample size=3000\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        parquet_file -- a string of parquet file's name\n",
    "    \n",
    "    Outputs:\n",
    "        taxi_df -- dataframe with columns of datetime, coordinates of pickup&dropoff and tip amount\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(parquet_file,engine='pyarrow')\n",
    "    df.set_index(pd.to_datetime(df[\"pickup_datetime\"]),inplace=True)\n",
    "    df = df[(df.passenger_count != 0) & (df.fare_amount > 0)]\n",
    "    df = df[[\"pickup_longitude\",\"pickup_latitude\",\"dropoff_longitude\",\"dropoff_latitude\",\"tip_amount\"]]\n",
    "    taxi_df=df.sample(n=3000,random_state=100)\n",
    "    return taxi_df\n",
    "def normal_before_2011(file):\n",
    "    \"\"\"Calculate Distance: remove rows with coordinates outside of new york box and add distance\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        file -- sampled dataframe \n",
    "    \n",
    "    Outputs:\n",
    "        taxi_df -- dataframe with columns of datetime, coordinates of pickup&dropoff, distance and tip amount\n",
    "    \"\"\"\n",
    "    if file[:4]==\"2009\":\n",
    "        taxi_df=norm_2009(file)\n",
    "    else:\n",
    "        taxi_df=norm_2010(file)\n",
    "    taxi_df=taxi_df.loc[(taxi_df[\"pickup_latitude\"]<=40.908524)&(taxi_df[\"pickup_latitude\"]>=40.560445)&(taxi_df[\"dropoff_latitude\"]<=40.908524)&(taxi_df[\"dropoff_latitude\"]>=40.560445)&(taxi_df[\"pickup_longitude\"]<=-73.717047)&(taxi_df[\"pickup_longitude\"]>=-74.242330)&(taxi_df[\"dropoff_longitude\"]<=-73.717047)&(taxi_df[\"dropoff_longitude\"]>=-74.242330)].copy()\n",
    "    add_distance = taxi_df.apply(\n",
    "        lambda row: calculate_distance(row[\"pickup_latitude\"], row[\"pickup_longitude\"],row[\"dropoff_latitude\"],row[\"dropoff_longitude\"]),axis=1)\n",
    "    taxi_df['distance'] = add_distance\n",
    "    taxi_df = taxi_df[taxi_df.distance > 0]\n",
    "    taxi_df.dropna(inplace = True)\n",
    "    return taxi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list storing all the file names of yellow taxi monthly data\n",
    "res=find_taxi_csv_urls()\n",
    "title=[]\n",
    "for x in res:\n",
    "    pattern = r\"(2009-\\d{2}|2010-\\d{2}|2011-\\d{2}|2012-\\d{2}|2013-\\d{2}|2014-\\d{2}|2015-0[1-6])\\.(parquet)\"\n",
    "    result = re.search(pattern, x)\n",
    "   \n",
    "    if result != None:\n",
    "        title.append(result.groups()[0]+\".parquet\")\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. Append Monthly Data to a Big Dataframe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(title):\n",
    "    \"\"\"Process Monthly Data and append to a single DataFrame\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        file -- sampled dataframe \n",
    "    \n",
    "    Outputs:\n",
    "        taxi_df -- dataframe with columns of datetime, coordinates of pickup&dropoff, distance and tip amount\n",
    "    \"\"\"\n",
    "    all_taxi_dataframes = []    \n",
    "    for urls in title:\n",
    "        if urls[:4] == \"2009\" or  urls[:4] == \"2010\":\n",
    "            all_taxi_dataframes.append(normal_before_2011(urls))\n",
    "        else:\n",
    "            all_taxi_dataframes.append(normal_from_2011(urls))\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    return taxi_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Taxi_Data=get_and_clean_taxi_data(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "\n",
    "##### 1. Manually downloaded and stored Uber data as \"uber_rides_sample.csv\"\n",
    "\n",
    "##### 2. Replace index with pickup_datetime\n",
    "\n",
    "##### 3. Remove invalid trips\n",
    "Trips outside the required coordinate box;\n",
    "Trips with zero passenger count;\n",
    "Trips with no fare;\n",
    "Trips with no distance between dropoff and pickup\n",
    "\n",
    "##### 4. Remove unnecessary columns\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;The dataset now only has 4 columns which represent longtitudes and latitudes respectively. \n",
    "##### 5. Add distance column\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Implemented calculate_distance function and add distance as a new column\n",
    "##### 6. Drop NaN & Normalize column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    \"\"\"Load and clean the Uber data.\n",
    "\n",
    "    Keyword arguments:\n",
    "    Inputs: \n",
    "        csv_file -- Uber data's file name\n",
    "    Output:\n",
    "        uber -- cleaned dataframe with columns of pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, and distance\n",
    "    \"\"\"\n",
    "    uber = pd.read_csv(csv_file) \n",
    "    uber.set_index(pd.to_datetime(uber['pickup_datetime']),inplace=True)\n",
    "    uber.drop([\"key\",\"Unnamed: 0\",'pickup_datetime'],axis=1,inplace=True)\n",
    "    uber = uber[(uber.passenger_count != 0) & (uber.fare_amount > 0)]\n",
    "    uber.drop([\"passenger_count\",\"fare_amount\"],axis=1,inplace=True)\n",
    "    uber['pickup_latitude'].apply(lambda x: float(x))\n",
    "    uber['pickup_longitude'].apply(lambda x: float(x))\n",
    "    uber['dropoff_latitude'].apply(lambda x: float(x))\n",
    "    uber['dropoff_longitude'].apply(lambda x: float(x))\n",
    "    uber=uber.loc[(uber[\"pickup_latitude\"]<=40.908524)&(uber[\"pickup_latitude\"]>=40.560445)&(uber[\"dropoff_latitude\"]<=40.908524)&(uber[\"dropoff_latitude\"]>=40.560445)&(uber[\"pickup_longitude\"]<=-73.717047)&(uber[\"pickup_longitude\"]>=-74.242330)&(uber[\"dropoff_longitude\"]<=-73.717047)&(uber[\"dropoff_longitude\"]>=-74.242330)]\n",
    "    add_distance = uber.apply(\n",
    "        lambda row: calculate_distance(row[\"pickup_latitude\"], row[\"pickup_longitude\"],row[\"dropoff_latitude\"],row[\"dropoff_longitude\"]),axis=1)\n",
    "    uber['distance'] = add_distance\n",
    "    uber = uber[uber.distance > 0]\n",
    "    uber.dropna(inplace = True)\n",
    "    \n",
    "    return uber\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Uber_Data=load_and_clean_uber_data(\"uber_rides_sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "\n",
    "For Weather Data, we manually downloaded them and stored as csv files. Then, we created two functions to manipulate hourly and daily data respectively. Moreover, within each function, we cleaned data by removing invalid data, removing unnecessary data, and transforming to appropriate column data type. After weather data processing, we got 2 dataframes. One is called Hourly_Weather_Data with index of datetime, columns of HourlyPrecipitation and HourlyWindSpeed. Another is called Daily_Weather_Data with index of datetime, columns of DailyAverageWindSpeed and DailyPrecipitation.\n",
    "\n",
    "##### 1. Clean hourly weather data\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *  Extract only useful columns: \"DATE\",\"HourlyPrecipitation\",and \"HourlyWindSpeed\"\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *  Replace \"T\" with 0 since it indicates trace amount of precipitation\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *  Convert HourlyPrecipitation data into numeric type since there are some wrongly denoted data such as \"0.12s\"\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *  Convert DATE to datetime type and replace the index with it\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *  Group the data into hourly data using resample('60min').mean() in pandas\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *  Construct cleaned dataframe \"Hourly_Weather_Data\" with index of DATE, columns of HourlyPrecipitation and HourlyWindSpeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    \"\"\"Process and Clean Hourly Weather Data\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        csv_file -- string of the name of weather file\n",
    "    Outputs:\n",
    "        weather -- Dataframe with columns of datetime, hourly precipitaion, and hourly wind speed\n",
    "    \n",
    "    \"\"\"\n",
    "    weather = pd.read_csv(csv_file,low_memory=False)\n",
    "    weather = weather[[\"DATE\",\"HourlyPrecipitation\",\"HourlyWindSpeed\"]]\n",
    "    weather.replace(to_replace='T',value=0,inplace=True)\n",
    "    weather['HourlyPrecipitation']=pd.to_numeric(weather['HourlyPrecipitation'],errors='coerce')\n",
    "    weather.set_index(pd.to_datetime(weather[\"DATE\"]),inplace=True)\n",
    "    weather=weather.resample('60min').mean()\n",
    "    weather = weather[[\"HourlyPrecipitation\",\"HourlyWindSpeed\"]]\n",
    "    return weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Clean daily weather data\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *  For data from 2009 to 2011, daily data is missing, so we calculate daily data from hourly data:\n",
    "<br> \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*Take the mean value for windspeed and take the sum for precipitation*\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *  Replace \"T\" with 0 since it indicates trace amount of precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    \"\"\"Process and Clean Daily Weather Data\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        csv_file -- string of the name of weather file\n",
    "    Outputs:\n",
    "        weather -- Dataframe with columns of date, hourly precipitaion, and hourly wind speed\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    weather=pd.read_csv(csv_file,low_memory=False)\n",
    "    weather=weather[[\"DATE\",\"DailyAverageWindSpeed\",\"DailyPrecipitation\",\"HourlyPrecipitation\",\"HourlyWindSpeed\"]]\n",
    "    weather.replace(to_replace='T',value=0,inplace=True)\n",
    "    if int(csv_file[:4])<2012:\n",
    "        weather=weather[[\"DATE\",\"HourlyPrecipitation\",\"HourlyWindSpeed\"]]\n",
    "        weather.set_index(pd.to_datetime(weather['DATE']),inplace=True)\n",
    "        weather[\"HourlyPrecipitation\"]=pd.to_numeric(weather[\"HourlyPrecipitation\"])\n",
    "        precipitation=weather[[\"DATE\",\"HourlyPrecipitation\"]].copy()\n",
    "        precipitation.dropna(inplace=True)\n",
    "        wind=weather[[\"DATE\",\"HourlyWindSpeed\"]].copy()\n",
    "        wind.dropna(inplace=True)\n",
    "        b=precipitation.HourlyPrecipitation.resample('D').sum()\n",
    "        a=wind.HourlyWindSpeed.resample('D').mean()\n",
    "        weather=pd.concat([a,b],axis=1)\n",
    "        weather.rename(columns={'HourlyWindSpeed': 'DailyAverageWindSpeed', 'HourlyPrecipitation': 'DailyPrecipitation'}, inplace=True)\n",
    "    else:\n",
    "        weather.set_index(pd.to_datetime(weather['DATE']).dt.date,inplace=True)\n",
    "        weather.index = pd.to_datetime(weather.index)\n",
    "        weather[\"DailyPrecipitation\"]=pd.to_numeric(weather[\"DailyPrecipitation\"])\n",
    "        weather=weather[[\"DailyAverageWindSpeed\",\"DailyPrecipitation\"]]\n",
    "        weather.dropna(subset=[\"DailyAverageWindSpeed\",\"DailyPrecipitation\"],inplace=True)\n",
    "    return weather\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Construct Hourly and Daily Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    \"\"\"Process all weather data and produce two dataframes\n",
    "    Key Arguments:\n",
    "    Outputs:\n",
    "        hourly_data -- Dataframe with columns of datetime, hourly precipitaion, and hourly wind speed\n",
    "        daily_data -- Dataframe with columns of date, hourly precipitaion, and hourly wind speed    \n",
    "    \"\"\"\n",
    "\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    \n",
    "    # add some way to find all weather CSV files\n",
    "    # or just add the name/paths manually\n",
    "    weather_csv_files = [\"2009_weather.csv\",\"2010_weather.csv\",\"2011_weather.csv\",\"2012_weather.csv\",\"2013_weather.csv\",\"2014_weather.csv\",\"2015_weather.csv\"]\n",
    "    \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hourly_Weather_Data, Daily_Weather_Data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data\n",
    "\n",
    "We use SQL statement to create 4 tables to store Yellow Taxi Data, Uber Data, Hourly Weather Data and Daily Weather Data. Then, we use pandas to insert data into tables, and the results are shown in dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(f\"sqlite:///project.db\", echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Hourly_Weather (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        date DATETIME,\n",
    "        hourlyprecipitation FLOAT,\n",
    "        hourlywindspeed FLOAT\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Daily_Weather (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        date DATE,\n",
    "        dailyaveragewindspeed FLOAT,\n",
    "        dailyprecipitation FLOAT\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Taxi_Trips (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        pickup_datetime DATETIME,\n",
    "        pickup_longitude FLOAT,\n",
    "        pickup_latitude FLOAT,\n",
    "        dropoff_longitude FLOAT,\n",
    "        dropoff_latitude FLOAT,\n",
    "        tip_amount FLOAT,\n",
    "        distance FLOAT\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Uber_Trips(\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        pickup_datetime DATETIME,\n",
    "        pickup_longitude FLOAT,\n",
    "        pickup_latitude FLOAT,\n",
    "        dropoff_longitude FLOAT,\n",
    "        dropoff_latitude FLOAT,\n",
    "        distance FLOAT\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open('schema.sql', \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(HOURLY_WEATHER_SCHEMA)\n",
    "    connection.execute(DAILY_WEATHER_SCHEMA)\n",
    "    connection.execute(TAXI_TRIPS_SCHEMA)\n",
    "    connection.execute(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Data to Database\n",
    "\n",
    "We use to_sql( ) from pandas to insert value in the tables that we created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Uber_Data.to_sql('Uber_Trips', con = engine, if_exists = 'append')\n",
    "Taxi_Data.to_sql('Taxi_Trips', con = engine, if_exists = 'append')\n",
    "Hourly_Weather_Data.to_sql('Hourly_Weather', con = engine, if_exists = 'append')\n",
    "Daily_Weather_Data.to_sql('Daily_Weather', con = engine, if_exists = 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_table(\"Uber_Trips\",engine).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_table(\"Taxi_Trips\",engine).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_table(\"Hourly_Weather\",engine).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_table(\"Daily_Weather\",engine).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Query 1: Group the amount of taxi trips by the hour of datetime then order them descendingly, so the first row is the most popular hour\n",
    "<br>\n",
    "Query 2: Group the amount of taxi trips by the weekday of datetime then order them descendingly, so the first row is the most popular day of week\n",
    "<br>\n",
    "Query 3: This query is divided into two parts: union the tables of uber & taxi and use LIMIT & OFFSET to get 95% percentile (order the data by distance, get the index of 95% percentile, and only return this row by LIMIT 1 and OFFSET the index)\n",
    "<br>\n",
    "Query 4: This query is divided into two parts: union the tables of uber & taxi for 2009 data and use LIMIT to extract the top 10 days with ride amounts & average distance(order the data by hired trips amount descendingly, so the first row has the highest number of hired rides for 2009)\n",
    "<br>\n",
    "Query 5: Use left join to combine weather and hired trips data. Then we calculate the average wind speed by using avg() function. Moreover, we used strftime() and count() function to extract date and to track corresponding hired trip amounts. We ordered the data by average wind speed descendingly, so the first row is the windest day for 2014. We used LIMIT to extract the windest 10 days in 2014\n",
    "<br>\n",
    "Query 6:Use left join to combine weather and hired trips data. During the data processing stage, we kept all the rows of hourly weather data even the value is NaN. Thus, we choose to use the weather table as the left table because each hour should have an entry. Then, we group the joined table by hour and use count() to get the amount of trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1 = \"\"\"\n",
    "    SELECT strftime ('%H',pickup_datetime) AS HOUR, count(id) AS Trip_Amount\n",
    "    FROM Taxi_Trips\n",
    "    GROUP BY strftime ('%H',pickup_datetime)\n",
    "    ORDER BY Trip_Amount DESC\n",
    "\"\"\"\n",
    "QUERY_2 = \"\"\"\n",
    "    SELECT strftime('%w',pickup_datetime) AS WeekDay, count(strftime('%w',pickup_datetime)) as Trip_Amount\n",
    "    FROM Uber_Trips\n",
    "    Group By WeekDay\n",
    "    ORDER BY Trip_Amount DESC\n",
    "\"\"\"\n",
    "QUERY_3=\"\"\"\n",
    "    SELECT\n",
    "      distance AS '95% percentile distance'\n",
    "    FROM (SELECT distance from Uber_Trips where strftime ('%Y',pickup_datetime)='2013' AND strftime ('%m',pickup_datetime)='07'\n",
    "        UNION ALL\n",
    "        SELECT distance from Taxi_Trips where strftime ('%Y',pickup_datetime)='2013' AND strftime ('%m',pickup_datetime)='07')\n",
    "    ORDER BY distance ASC\n",
    "    LIMIT 1\n",
    "    OFFSET (SELECT\n",
    "             COUNT(*)\n",
    "            FROM(SELECT distance from Uber_Trips where strftime ('%Y',pickup_datetime)='2013' AND strftime ('%m',pickup_datetime)='07'\n",
    "        UNION ALL\n",
    "        SELECT distance from Taxi_Trips where strftime ('%Y',pickup_datetime)='2013' AND strftime ('%m',pickup_datetime)='07')\n",
    "          ) * 95 / 100 - 1;\n",
    "\"\"\"\n",
    "QUERY_4 = \"\"\"\n",
    "    SELECT strftime('%Y-%m-%d',pickup_datetime) AS Day, count(strftime('%d',pickup_datetime)) as Trip_Amount, sum(distance)/count(strftime('%d',pickup_datetime)) as Avg_Distance\n",
    "    FROM (SELECT distance,pickup_datetime from Uber_Trips where strftime ('%Y',pickup_datetime)='2009'\n",
    "            UNION ALL\n",
    "        SELECT distance,pickup_datetime from Taxi_Trips where strftime ('%Y',pickup_datetime)='2009')\n",
    "    Group By Day\n",
    "    Order By Trip_Amount DESC\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "QUERY_5 = \"\"\"\n",
    "    SELECT strftime('%Y-%m-%d',Hourly_Weather.date) as Day,avg(hourlywindspeed) as Wind_Speed,count(tbl.Time) as Trip_Amount\n",
    "    FROM Hourly_Weather \n",
    "    LEFT JOIN (SELECT strftime('%Y-%m-%d',pickup_datetime) as Time from Uber_Trips where strftime ('%Y',pickup_datetime)='2014'\n",
    "                    UNION ALL\n",
    "            SELECT strftime('%Y-%m-%d',pickup_datetime) as Time from Taxi_Trips where strftime ('%Y',pickup_datetime)='2014') AS tbl ON strftime('%Y-%m-%d',Hourly_Weather.date) = tbl.Time\n",
    "        \n",
    "    WHERE strftime ('%Y',Hourly_Weather.date)='2014'\n",
    "    Group by Day\n",
    "    Order By Wind_Speed DESC\n",
    "    LIMIT 10   \n",
    "\"\"\"\n",
    "QUERY_6=\"\"\"\n",
    "    Select strftime('%Y-%m-%d %H',Hourly_Weather.date) AS Hour_in_Date,Hourly_Weather.hourlyprecipitation AS Precipitation,Hourly_Weather.hourlywindspeed AS SustainedWindSpeed,count(tbl.pickup_datetime) AS Trip_Amount\n",
    "    FROM Hourly_Weather\n",
    "    left join (SELECT pickup_datetime,distance from Uber_Trips WHERE pickup_datetime BETWEEN '2012-10-22 00:00:00' AND '2012-11-06 00:00:00'\n",
    "            UNION ALL\n",
    "            SELECT pickup_datetime,distance from Taxi_Trips where pickup_datetime BETWEEN '2012-10-22 00:00:00' AND '2012-11-06 00:00:00')AS tbl\n",
    "    on strftime('%Y-%m-%d %H',tbl.pickup_datetime)= strftime('%Y-%m-%d %H',Hourly_Weather.date)\n",
    "    where Hourly_Weather.date BETWEEN '2012-10-22 00:00:00' AND '2012-11-06 00:00:00'\n",
    "    GROUP BY strftime('%Y-%m-%d %H',Hourly_Weather.date)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"most_popular_hour_taxi.sql\", \"w\") as f:\n",
    "    f.write(QUERY_1)\n",
    "with open(\"most_popular_weekday.sql\", \"w\") as f:\n",
    "    f.write(QUERY_2)\n",
    "with open(\"95%_percentile_distance_2013_07.sql\", \"w\") as f:\n",
    "    f.write(QUERY_3)\n",
    "with open(\"2009_top10_most_ride.sql\", \"w\") as f:\n",
    "    f.write(QUERY_4)\n",
    "with open(\"2014_top10_windest_rides.sql\", \"w\") as f:\n",
    "    f.write(QUERY_5)\n",
    "with open(\"hourly_weather_trips.sql\", \"w\") as f:\n",
    "    f.write(QUERY_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result 1: 7PM is the most popular hour to take yellow taxi with trip amount of 14082__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(QUERY_1,con=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result 2: For 01-2009 through 06-2015, Friday was the most popular to take an uber.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(QUERY_2,con=engine) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result 3: 95% percentile distance is 10.136294__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(QUERY_3,con=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result 4: Top 10 days with the highest number of hired rides and average distance for 2009 are shown below__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(QUERY_4,con=engine) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result 5: The windiest 10 days in 2014 and corresponding hired trip amounts are shown below__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(QUERY_5,con=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result 6: There are 360 hours in the required time range.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(QUERY_6,con=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use bar, line, and scatter plot for this part to create readable and interpretable visualizations based on the data from SQL tables. Furthermore, we enhanced the second plot by creating an animation of the line that represents the average distance traveled per month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.  Ordered Amount of Trips of Yellow Taxi in Each Hour__\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Bar graph can convey the difference of each hour effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_1(data):\n",
    "    \"\"\"Create bar graph for amount of trips in each hour\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        data -- results after executing query 1    \n",
    "    \"\"\"\n",
    "    x_axis=[]\n",
    "    y_axis=[]\n",
    "    for m in data_1:\n",
    "        x_axis.append(m[0])\n",
    "        y_axis.append(m[1])\n",
    "    fig = plt.figure(figsize =(12, 6))\n",
    "    plt.bar(x_axis,y_axis,color = \"lightblue\")\n",
    "    plt.title('Hourly Amount of Trips of Yellow Taxi From 2009 to 2015',fontsize=20,fontweight=\"bold\")\n",
    "    plt.xlabel('Hour',fontsize=15)\n",
    "    plt.ylabel('Amount of Trips',fontsize=15)\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1=engine.execute(QUERY_1).fetchall()\n",
    "plot_visual_1(data_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.  Average Distance Traveled Per Month & 90% Confidence Interval around the Mean__\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Since the average distance traveled in each month only appears small changes, line graph could track trivial changes over periods of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 plot\n",
    "def get_data_for_visual_2():\n",
    "    \"\"\"Creating dataframes from querying the SQL table\n",
    "    Key Arguments:\n",
    "    Outputs:\n",
    "        VQUERY_2 -- SQL Query that select average distance per month\n",
    "    \"\"\"\n",
    "    \n",
    "    VQUERY_2 = \"\"\"\n",
    "        SELECT strftime('%m',pickup_datetime) AS Month, sum(distance)/count(strftime('%m',pickup_datetime)) as Avg_Distance\n",
    "        FROM (SELECT distance,pickup_datetime from Uber_Trips\n",
    "                UNION ALL\n",
    "            SELECT distance,pickup_datetime from Taxi_Trips) \n",
    "        Group By Month\n",
    "    \"\"\"\n",
    "    return VQUERY_2\n",
    "    \n",
    "def plot_visual_2(avg_dis):\n",
    "    \"\"\"Create line graph and calculate 90% CI\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        avg_dis -- dataframe after querying    \n",
    "    \"\"\"\n",
    "    x = list(zip(*avg_dis))[0]\n",
    "    y = list(zip(*avg_dis))[1]\n",
    "\n",
    "    SE = sem(y)\n",
    "    Mean = sum(y) / len(y)\n",
    "    CI = (Mean-1.68*SE,Mean+1.68*SE)\n",
    "    y1 = CI[0]\n",
    "    y2 = CI[1]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(x,y,marker='o',markersize = \"9\",linewidth='4')\n",
    "    plt.axhline(y = y1, color = 'grey', linestyle = '--')\n",
    "    plt.axhline(y = y2, color = 'grey', linestyle = '--')\n",
    "    plt.fill_between(x, y1,y2,facecolor='lightblue',alpha=0.3)\n",
    "    plt.title('Average Distance Traveled per Month',size=20,fontweight=\"bold\")\n",
    "    plt.xlabel('Month',fontsize=15)\n",
    "    plt.ylabel('Average Distance',fontsize=15)\n",
    "\n",
    "    Dot_Line = mpatches.Patch(color='lightblue', label='90% CI')\n",
    "    Line = mpatches.Patch(color='lightblue', label='90% CI')\n",
    "    plt.legend(handles=[Dot_Line],fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdata2 = get_data_for_visual_2()\n",
    "avg_dis = engine.execute(vdata2).fetchall()\n",
    "plot_visual_2(avg_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3.  Amount of Trips Near 3 Airports Group By Day of the Week__\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Plotting 3 lines with dots that represent 3 different airports enables us to see the difference and trend effectively.\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; The coordinates of 3 airports are listed below:\n",
    "<br>\n",
    "* EWR = -74.198488,40.670700,-74.152311,40.708577\n",
    "<br>\n",
    "* LGA = -73.890082,40.766904,-73.855235,40.783024\n",
    "<br>\n",
    "* JFK = -73.819542,40.630891,-73.756371,40.665275\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_3():\n",
    "    query_ewr=\"\"\"\n",
    "                SELECT tbl.day, count(*)\n",
    "                FROM (SELECT id,strftime('%w',pickup_datetime) AS day from Uber_Trips where (dropoff_longitude BETWEEN -74.198488 AND -74.152311) AND (dropoff_latitude BETWEEN 40.670700 AND 40.708577)\n",
    "                      UNION ALL\n",
    "                      SELECT id,strftime('%w',pickup_datetime) AS day from Taxi_Trips where (dropoff_longitude BETWEEN -74.198488 AND -74.152311) AND (dropoff_latitude BETWEEN 40.670700 AND 40.708577)) as tbl\n",
    "                GROUP BY tbl.day\n",
    "            \"\"\"\n",
    "    query_lga=\"\"\"\n",
    "                SELECT tbl.day, count(*)\n",
    "                FROM (SELECT id,strftime('%w',pickup_datetime) AS day from Uber_Trips where (dropoff_longitude BETWEEN -73.890082 AND -73.855235) AND (dropoff_latitude BETWEEN 40.766904 AND 40.783024)\n",
    "                      UNION ALL\n",
    "                      SELECT id,strftime('%w',pickup_datetime) AS day from Taxi_Trips where (dropoff_longitude BETWEEN -73.890082 AND -73.855235) AND (dropoff_latitude BETWEEN 40.766904 AND 40.783024)) as tbl\n",
    "                GROUP BY tbl.day\n",
    "            \"\"\"\n",
    "    query_jfk=\"\"\"\n",
    "                SELECT tbl.day, count(*)\n",
    "                FROM (SELECT id,strftime('%w',pickup_datetime) AS day from Uber_Trips where (dropoff_longitude BETWEEN -73.819542 AND -73.756371) AND (dropoff_latitude BETWEEN 40.630891 AND 40.665275)\n",
    "                      UNION ALL\n",
    "                      SELECT id,strftime('%w',pickup_datetime) AS day from Taxi_Trips where (dropoff_longitude BETWEEN -73.819542 AND -73.756371) AND (dropoff_latitude BETWEEN 40.630891 AND 40.665275)) as tbl\n",
    "                GROUP BY tbl.day\n",
    "            \"\"\"\n",
    "    ewr=engine.execute(query_ewr).fetchall()\n",
    "    lga=engine.execute(query_lga).fetchall()\n",
    "    jfk=engine.execute(query_jfk).fetchall()\n",
    "    return ewr,lga,jfk\n",
    " \n",
    "\n",
    "def get_y_axis(coor):\n",
    "    \"\"\"Get Amount of trips from query execution result\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        coor --results from query execution\n",
    "    \"\"\"\n",
    "    y=[]\n",
    "    for m in coor:\n",
    "        y.append(m[1])\n",
    "    return y\n",
    "def plot_visual_3(ewr,lga,jfk):\n",
    "    \"\"\"Create line graph with dots\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        ewr -- results from query of EWR data\n",
    "        lga -- results from query of LGA data\n",
    "        jfk -- results from query of JFK data\n",
    "    \"\"\"\n",
    "    x=['SUN','MON','TUE','WED','THU','FRI','SAT']\n",
    "    ewr= get_y_axis(ewr)     \n",
    "    lga= get_y_axis(lga) \n",
    "    jfk= get_y_axis(jfk) \n",
    "    fig = plt.figure(figsize =(12, 6))\n",
    "    plt.plot(x,ewr,'lightblue',label=\"EWR\",marker='o',markersize = \"9\",linewidth='4')\n",
    "    plt.plot(lga,'lightcoral',label=\"LGA\",marker='o',markersize = \"9\",linewidth='4')\n",
    "    plt.plot(jfk,'lightseagreen',label=\"JFK\",marker='o',markersize = \"9\",linewidth='4')\n",
    "    plt.title('Amount of Trips Near New York Airports',size=20,fontweight=\"bold\")\n",
    "    plt.xlabel('Day of Week',fontsize=15)\n",
    "    plt.ylabel('Amount of Trips',fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewr,lga,jfk=get_data_for_visual_3()\n",
    "plot_visual_3(ewr,lga,jfk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
