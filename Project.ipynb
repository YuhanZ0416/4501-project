{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "## Introduction:\n",
    "\n",
    "Uber announced that its users in New York City could order yellow taxis through the Uber app in the future. To explore the trends for Uber and yellow taxi, we make analysis based on hired-ride trip data from Uber and NYC Yellow cab from January 2009 through June 2015, and local historical weather data.\n",
    "##### The analysis is mainly broken up into 4 Parts: (Detailed analysis about each part are shown in the following Jupyter Notebook)\n",
    "<br>\n",
    "Data Preprocessing\n",
    "<br>\n",
    "Storing Data\n",
    "<br>\n",
    "Understanding Data\n",
    "<br>\n",
    "Visualizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup\n",
    "All import statements needed for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "from scipy.stats import sem\n",
    "from keplergl import KeplerGl\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "import geopandas as gpd\n",
    "import re\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.animation import FuncAnimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating distance\n",
    "Define a functin called \"calculate_distance\" that calculates the distance between two coordinates in kilometers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(from_lat, from_long,to_lat,to_long):\n",
    "    \"\"\"Calculate the distance bewteen two coordinates in kilometers.\n",
    "\n",
    "    Keyword arguments:\n",
    "    Inputs: \n",
    "        from_lat -- first coordinate's latitude\n",
    "        from_long -- first coordinate's longitude\n",
    "        to_lat -- second coordinate's latitude\n",
    "        to_long -- second coordinate's longitude\n",
    "    Output:\n",
    "        distance -- distance between two coordinates in kilometers\n",
    "    \"\"\"\n",
    "    \n",
    "    R = 6373.0\n",
    "    lat1 = radians(from_lat)\n",
    "    lon1 = radians(from_long)\n",
    "    lat2 = radians(to_lat)\n",
    "    lon2 = radians(to_long)\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Taxi Data\n",
    "\n",
    "For the taxi data, we first downloaded parquet files programmingly using regular expression,requests and beautifulsoup module. Then, we constructed a sampling of 3000 rows for each month data to make the sample size consistent with the one of Uber Data. The next step is cleaning data including removing invalid data, unnecessary columns, and add a column of distance according to the coordinates. Lastly, we append data of each month to a big dataframe.\n",
    "<br>\n",
    "\n",
    "##### Invalid Data Criteria:\n",
    "passenger count=0\n",
    "<br>\n",
    "fare amount<=0\n",
    "<br>\n",
    "distance <=0\n",
    "<br>\n",
    "coordinates out of New York box or NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Find Urls of Taxi Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_csv_urls():\n",
    "    \"\"\"Get Urls using requests and beautifulsoup\n",
    "    Keyword Arguments:\n",
    "    Output:\n",
    "        res -- A list contain all urls of yellow taxi montly data\n",
    "    \n",
    "    \"\"\"\n",
    "    TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "    response = requests.get(TAXI_URL)\n",
    "    html = response.content\n",
    "    soup = bs4.BeautifulSoup(html,'html.parser')\n",
    "    w=soup.find_all(\"a\")\n",
    "    res=[]\n",
    "    for i in range(len(w)):\n",
    "        if w[i].text==\"Yellow Taxi Trip Records\":\n",
    "            res.append(w[i]['href'])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Download Taxi Data from 2009-01 to 2015-06__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=find_taxi_csv_urls()\n",
    "for x in res:\n",
    "    #Use regular expression to extract required urls and use requests to download data\n",
    "    pattern = r\"(2009-\\d{2}|2010-\\d{2}|2011-\\d{2}|2012-\\d{2}|2013-\\d{2}|2014-\\d{2}|2015-0[1-6])\\.(parquet)\"\n",
    "    result = re.search(pattern, x)\n",
    "   \n",
    "    if result != None:\n",
    "        response = requests.get(x, stream=True)\n",
    "        title=result.groups()[0]\n",
    "    with open(title+\".parquet\", \"wb\") as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Clean Data from 2011 to 2015 (Location ID is provided instead of Coordinates)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = gpd.read_file('taxi_zones.shp')\n",
    "df2 = df2.to_crs(epsg=4326)  # EPSG 4326 = WGS84 = https://epsg.io/4326\n",
    "def find_long(ID,df2):\n",
    "    \"\"\"Find longitude using taxi_zones.shp corresponding to the Location ID\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        ID -- Location ID\n",
    "        df2 -- dataframe of taxi_zones.shp\n",
    "    \n",
    "    Outputs:\n",
    "        long-- longitude\n",
    "        np.nan-- if the coordinate is out of new york box\n",
    "    \"\"\"\n",
    "    long=df2.iloc[ID-1].geometry.centroid.x\n",
    "    if -74.242330<=long<=-73.717047:\n",
    "        return long\n",
    "    else:\n",
    "        return np.nan\n",
    "def find_lat(ID,df2):\n",
    "    \"\"\"Find latitude using taxi_zones.shp corresponding to the Location ID\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        ID -- Location ID\n",
    "        df2 -- dataframe of taxi_zones.shp \n",
    "    \n",
    "    Outputs:\n",
    "        lat-- latitude\n",
    "        np.nan-- if the coordinate is out of new york box\n",
    "    \"\"\"\n",
    "    lat=df2.iloc[ID-1].geometry.centroid.y\n",
    "    if 40.560445<=lat<=40.908524:\n",
    "        return lat\n",
    "    else:\n",
    "        return np.nan\n",
    "def normal_from_2011(parquet_file):\n",
    "    \"\"\"Clean Data: Normalize column names, remove invalid rows, sampling size=3000, find coordinates and add distance\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        parquet_file -- a string of parquet file's name\n",
    "    \n",
    "    Outputs:\n",
    "        taxi_df -- dataframe with columns of datetime, coordinates of pickup&dropoff, distance, and tip amount\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_parquet(parquet_file,engine='pyarrow')\n",
    "    df = df[(df.passenger_count != 0) & (df.fare_amount > 0)]\n",
    "    df.rename(columns={'tpep_pickup_datetime':'pickup_datetime'},inplace=True)    \n",
    "    df.set_index(pd.to_datetime(df[\"pickup_datetime\"]),inplace=True)\n",
    "    df = df[[\"PULocationID\", \"DOLocationID\",\"tip_amount\"]]\n",
    "    taxi_df=df.sample(n=3000,random_state=100)\n",
    "    taxi_df = taxi_df.loc[(taxi_df['PULocationID'] < 264) & (taxi_df['PULocationID'] >= 1)]\n",
    "    taxi_df = taxi_df.loc[(taxi_df['DOLocationID'] < 264) & (taxi_df['DOLocationID'] >= 1)]\n",
    "    \n",
    "    taxi_df[\"pickup_latitude\"]=np.nan\n",
    "    taxi_df[\"pickup_longitude\"]=np.nan\n",
    "    taxi_df[\"dropoff_latitude\"]=np.nan\n",
    "    taxi_df[\"dropoff_longitude\"]=np.nan\n",
    "    lat1 = taxi_df.apply(\n",
    "        lambda row: find_lat(row[\"PULocationID\"].astype('int'),df2),axis=1)\n",
    "    long1 = taxi_df.apply(\n",
    "        lambda row: find_long(row[\"PULocationID\"].astype('int'),df2),axis=1)\n",
    "    \n",
    "    taxi_df[\"pickup_latitude\"] = lat1\n",
    "    taxi_df[\"pickup_longitude\"] = long1\n",
    " \n",
    "    lat2 = taxi_df.apply(\n",
    "        lambda row: find_lat(row[\"DOLocationID\"].astype('int'),df2),axis=1)\n",
    "    long2 = taxi_df.apply(\n",
    "    lambda row: find_long(row[\"DOLocationID\"].astype('int'),df2),axis=1)\n",
    "\n",
    "    \n",
    "    taxi_df[\"dropoff_latitude\"] = lat2\n",
    "    taxi_df[\"dropoff_longitude\"] = long2   \n",
    "    \n",
    "    add_distance = taxi_df.apply(\n",
    "                 lambda row: calculate_distance(row[\"pickup_latitude\"], row[\"pickup_longitude\"],row[\"dropoff_latitude\"],row[\"dropoff_longitude\"]),\n",
    "                 axis=1)\n",
    "    taxi_df['distance'] = add_distance\n",
    "    taxi_df = taxi_df[taxi_df.distance > 0]\n",
    "    taxi_df.dropna(inplace=True)\n",
    "    \n",
    "    return pd.DataFrame(taxi_df, columns=[\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\",\"dropoff_latitude\",\"tip_amount\",\"distance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. Clean Data of 2009 and 2010 Respectively__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_2009(file):\n",
    "    \"\"\"Clean Data: extract necessary columns and normalize names, remove invalid rows, sample size=3000\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        file -- a string of parquet file's name\n",
    "    \n",
    "    Outputs:\n",
    "        par -- dataframe with columns of datetime, coordinates of pickup&dropoff and tip amount\n",
    "    \"\"\"\n",
    "    par=pd.read_parquet(file,engine='pyarrow')\n",
    "    par.rename(columns={'Trip_Pickup_DateTime':'pickup_datetime'},inplace=True)\n",
    "    par.set_index(pd.to_datetime(par['pickup_datetime']),inplace=True)\n",
    "    par = par[(par.Passenger_Count != 0) & (par.Fare_Amt > 0)]\n",
    "    par=par[['Start_Lon','Start_Lat','End_Lon','End_Lat','Tip_Amt']]\n",
    "    par=par.sample(3000,random_state=100)\n",
    "    par.rename(columns={'Start_Lon': 'pickup_longitude', 'Start_Lat': 'pickup_latitude','End_Lon':'dropoff_longitude','End_Lat':'dropoff_latitude','Tip_Amt':'tip_amount'},inplace=True)\n",
    "    return par\n",
    "def norm_2010(parquet_file): \n",
    "    \"\"\"Clean Data: extract necessary columns and normalize names, remove invalid rows, sample size=3000\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        parquet_file -- a string of parquet file's name\n",
    "    \n",
    "    Outputs:\n",
    "        taxi_df -- dataframe with columns of datetime, coordinates of pickup&dropoff and tip amount\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(parquet_file,engine='pyarrow')\n",
    "    df.set_index(pd.to_datetime(df[\"pickup_datetime\"]),inplace=True)\n",
    "    df = df[(df.passenger_count != 0) & (df.fare_amount > 0)]\n",
    "    df = df[[\"pickup_longitude\",\"pickup_latitude\",\"dropoff_longitude\",\"dropoff_latitude\",\"tip_amount\"]]\n",
    "    taxi_df=df.sample(n=3000,random_state=100)\n",
    "    return taxi_df\n",
    "def normal_before_2011(file):\n",
    "    \"\"\"Calculate Distance: remove rows with coordinates outside of new york box and add distance\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        file -- sampled dataframe \n",
    "    \n",
    "    Outputs:\n",
    "        taxi_df -- dataframe with columns of datetime, coordinates of pickup&dropoff, distance and tip amount\n",
    "    \"\"\"\n",
    "    if file[:4]==\"2009\":\n",
    "        taxi_df=norm_2009(file)\n",
    "    else:\n",
    "        taxi_df=norm_2010(file)\n",
    "    taxi_df=taxi_df.loc[(taxi_df[\"pickup_latitude\"]<=40.908524)&(taxi_df[\"pickup_latitude\"]>=40.560445)&(taxi_df[\"dropoff_latitude\"]<=40.908524)&(taxi_df[\"dropoff_latitude\"]>=40.560445)&(taxi_df[\"pickup_longitude\"]<=-73.717047)&(taxi_df[\"pickup_longitude\"]>=-74.242330)&(taxi_df[\"dropoff_longitude\"]<=-73.717047)&(taxi_df[\"dropoff_longitude\"]>=-74.242330)].copy()\n",
    "    add_distance = taxi_df.apply(\n",
    "        lambda row: calculate_distance(row[\"pickup_latitude\"], row[\"pickup_longitude\"],row[\"dropoff_latitude\"],row[\"dropoff_longitude\"]),axis=1)\n",
    "    taxi_df['distance'] = add_distance\n",
    "    taxi_df = taxi_df[taxi_df.distance > 0]\n",
    "    taxi_df.dropna(inplace = True)\n",
    "    return taxi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list storing all the file names of yellow taxi monthly data\n",
    "res=find_taxi_csv_urls()\n",
    "title=[]\n",
    "for x in res:\n",
    "    pattern = r\"(2009-\\d{2}|2010-\\d{2}|2011-\\d{2}|2012-\\d{2}|2013-\\d{2}|2014-\\d{2}|2015-0[1-6])\\.(parquet)\"\n",
    "    result = re.search(pattern, x)\n",
    "   \n",
    "    if result != None:\n",
    "        title.append(result.groups()[0]+\".parquet\")\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. Append Monthly Data to a Big Dataframe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(title):\n",
    "    \"\"\"Process Monthly Data and append to a single DataFrame\n",
    "    Key Arguments:\n",
    "    Inputs:\n",
    "        file -- sampled dataframe \n",
    "    \n",
    "    Outputs:\n",
    "        taxi_df -- dataframe with columns of datetime, coordinates of pickup&dropoff, distance and tip amount\n",
    "    \"\"\"\n",
    "    all_taxi_dataframes = []    \n",
    "    for urls in title:\n",
    "        if urls[:4] == \"2009\" or  urls[:4] == \"2010\":\n",
    "            all_taxi_dataframes.append(normal_before_2011(urls))\n",
    "        else:\n",
    "            all_taxi_dataframes.append(normal_from_2011(urls))\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    return taxi_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Taxi_Data=get_and_clean_taxi_data(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "\n",
    "__1. Manually downloaded and stored Uber data as \"uber_rides_sample.csv\"__\n",
    "<br>\n",
    "__2. Replace index with pickup_datetime__\n",
    "<br>\n",
    "__3. Remove invalid trips__\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; * Trips outside the required coordinate box\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; * Trips with zero passenger count\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; * Trips with no fare\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; * Trips with no distance between dropoff and pickup\n",
    "<br>\n",
    "__4. Remove unnecessary columns__\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;The dataset now only has 4 columns which represent longtitudes and latitudes respectively. \n",
    "<br>\n",
    "__5. Add distance column__\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Implemented calculate_distance function and add distance as a new column\n",
    "<br>\n",
    "__6. Drop NaN & Normalize column names__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    \"\"\"Load and clean the Uber data.\n",
    "\n",
    "    Keyword arguments:\n",
    "    Inputs: \n",
    "        csv_file -- Uber data's file name\n",
    "    Output:\n",
    "        uber -- cleaned dataframe with columns of pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, and distance\n",
    "    \"\"\"\n",
    "    uber = pd.read_csv(csv_file) \n",
    "    uber.set_index(pd.to_datetime(uber['pickup_datetime']),inplace=True)\n",
    "    uber.drop([\"key\",\"Unnamed: 0\",'pickup_datetime'],axis=1,inplace=True)\n",
    "    uber = uber[(uber.passenger_count != 0) & (uber.fare_amount > 0)]\n",
    "    uber.drop([\"passenger_count\",\"fare_amount\"],axis=1,inplace=True)\n",
    "    uber['pickup_latitude'].apply(lambda x: float(x))\n",
    "    uber['pickup_longitude'].apply(lambda x: float(x))\n",
    "    uber['dropoff_latitude'].apply(lambda x: float(x))\n",
    "    uber['dropoff_longitude'].apply(lambda x: float(x))\n",
    "    uber=uber.loc[(uber[\"pickup_latitude\"]<=40.908524)&(uber[\"pickup_latitude\"]>=40.560445)&(uber[\"dropoff_latitude\"]<=40.908524)&(uber[\"dropoff_latitude\"]>=40.560445)&(uber[\"pickup_longitude\"]<=-73.717047)&(uber[\"pickup_longitude\"]>=-74.242330)&(uber[\"dropoff_longitude\"]<=-73.717047)&(uber[\"dropoff_longitude\"]>=-74.242330)]\n",
    "    add_distance = uber.apply(\n",
    "        lambda row: calculate_distance(row[\"pickup_latitude\"], row[\"pickup_longitude\"],row[\"dropoff_latitude\"],row[\"dropoff_longitude\"]),axis=1)\n",
    "    uber['distance'] = add_distance\n",
    "    uber = uber[uber.distance > 0]\n",
    "    uber.dropna(inplace = True)\n",
    "    \n",
    "    return uber\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Uber_Data=load_and_clean_uber_data(\"uber_rides_sample.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
